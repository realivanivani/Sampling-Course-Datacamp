{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2a8f64c",
   "metadata": {},
   "source": [
    "## Introduction to bootstrapping\n",
    "1. Introduction to bootstrapping\n",
    "\n",
    "So far, we've mostly focused on the idea of sampling without replacement.\n",
    "\n",
    "2. With or without\n",
    "\n",
    "Sampling without replacement is like dealing a pack of cards. When we deal the ace of spades to one player, we can't then deal the ace of spades to another player. Sampling with replacement is like rolling dice. If we roll a six, we can still get a six on the next roll. Sampling with replacement is sometimes called resampling. We'll use the terms interchangeably.\n",
    "\n",
    "3. Simple random sampling without replacement\n",
    "\n",
    "If we take a simple random sample without replacement, each row of the dataset, or each type of coffee, can only appear once in the sample.\n",
    "\n",
    "4. Simple random sampling with replacement\n",
    "\n",
    "If we sample with replacement, it means that each row of the dataset, or each coffee, can be sampled multiple times.\n",
    "\n",
    "5. Why sample with replacement?\n",
    "\n",
    "So far, we've been treating the coffee_ratings dataset as the population of all coffees. Of course, it doesn't include every coffee in the world, so we could treat the coffee dataset as just being a big sample of coffees. To imagine what the whole population is like, we need to approximate the other coffees that aren't in the dataset. Each of the coffees in the sample dataset will have properties that are representative of the coffees that we don't have. Resampling lets us use the existing coffees to approximate those other theoretical coffees.\n",
    "\n",
    "6. Coffee data preparation\n",
    "\n",
    "To keep it simple, let's focus on three columns of the coffee dataset. To make it easier to see which rows ended up in the sample, we'll add a row index column called index using the reset_index method.\n",
    "\n",
    "7. Resampling with .sample()\n",
    "\n",
    "To sample with replacement, we call sample as usual but set the replace argument to True. Setting frac to 1 produces a sample of the same size as the original dataset.\n",
    "\n",
    "    > coffee_resamp = coffee_focus.sample(frac=1, replace=True)\n",
    "    \n",
    "8. Repeated coffees\n",
    "\n",
    "Counting the values of the index column shows how many times each coffee ended up in the resampled dataset. Some coffees were sampled four or five times.\n",
    "\n",
    "9. Missing coffees\n",
    "\n",
    "That means that some coffees didn't end up in the resample. By taking the number of distinct index values in the resampled dataset, using len on drop_duplicates, we see that eight hundred and sixty-eight different coffees were included. By comparing this number with the total number of coffees, we can see that four hundred and seventy coffees weren't included in the resample.\n",
    "\n",
    "    > num_unique_coffees = len(coffee_resamp.drop_duplicates(subset=\"index\"))\n",
    "    \n",
    "10. Bootstrapping\n",
    "\n",
    "We're going to use resampling for a technique called bootstrapping. In some sense, bootstrapping is the opposite of sampling from a population. With sampling, we treat the dataset as the population and move to a smaller sample. **With bootstrapping, we treat the dataset as a sample and use it to build up a theoretical population.** A use case of bootstrapping is to try to understand the variability due to sampling. This is important in cases where we aren't able to sample the population multiple times to create a sampling distribution.\n",
    "\n",
    "\n",
    "11. Bootstrapping process\n",
    "\n",
    "The bootstrapping process has three steps. First, randomly sample with replacement to get a resample the same size as the original dataset. Then, calculate a statistic, such as a mean of one of the columns. Note that the mean isn't always the choice here and bootstrapping allows for complex statistics to be computed, too. Then, replicate this many times to get lots of these bootstrap statistics. Earlier in the course, we did something similar. We took a simple random sample, then calculated a summary statistic, then repeated those two steps to form a sampling distribution. This time, when we've used resampling instead of sampling, we get a bootstrap distribution.\n",
    "\n",
    "12. Bootstrapping coffee mean flavor\n",
    "\n",
    "The resampling step uses the code we just saw: calling sample with frac set to one and replace set to True. Calculating a bootstrap statistic can be done with mean from NumPy. In this case, we're calculating the mean flavor score. To repeat steps one and two one thousand times, we can wrap the code in a for loop and append the statistics to a list.\n",
    "\n",
    "    > np.mean(coffee_sample.sample(frac=1, replace=True)['flavor']\n",
    "    \n",
    "13. Bootstrap distribution histogram\n",
    "\n",
    "Here's a histogram of the bootstrap distribution of the sample mean. Notice that it is close to following a normal distribution.\n",
    "\n",
    "14. Let's practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05232d5a",
   "metadata": {},
   "source": [
    "*The key to deciding whether to sample without or with replacement is whether or not your dataset is best thought of as being the whole population or not.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f341a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 1 bootstrap resample\n",
    "spotify_1_resample = spotify_sample.sample(frac=1, replace=True)\n",
    "\n",
    "# Calculate of the danceability column of spotify_1_resample\n",
    "mean_danceability_1 = np.mean(spotify_1_resample['danceability'])\n",
    "\n",
    "# Replicate this 1000 times\n",
    "mean_danceability_1000 = []\n",
    "for i in range(1000):\n",
    "\tmean_danceability_1000.append(\n",
    "        np.mean(spotify_sample.sample(frac=1, replace=True)['danceability'])\n",
    "\t)\n",
    "\n",
    "# Draw a histogram of the resample means\n",
    "plt.hist(mean_danceability_1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61036006",
   "metadata": {},
   "source": [
    "## Comparing sampling and bootstrap distributions\n",
    "3. The bootstrap of mean coffee flavors\n",
    "\n",
    "Here, we generate a bootstrap distribution of the mean coffee flavor scores from that sample. dot-sample generates a resample, np-dot-mean calculates the statistic, and the for loop with dot-append repeats these steps to produce a distribution of bootstrap statistics.\n",
    "\n",
    "4. Mean flavor bootstrap distribution\n",
    "\n",
    "Here's the histogram of the bootstrap distribution, which is close to a normal distribution.\n",
    "\n",
    "5. Sample, bootstrap distribution, population means\n",
    "\n",
    "Here's the mean flavor score from the original sample. In the bootstrap distribution, each value is an estimate of the mean flavor score. Recall that each of these values corresponds to one potential sample mean from the theoretical population. If we take the mean of those means, we get our best guess of the population mean. The two values are really close. However, there's a problem. The true population mean is actually a little different.\n",
    "\n",
    "6. Interpreting the means\n",
    "\n",
    "The behavior that you just saw is typical. The bootstrap distribution mean is usually almost identical to the original sample mean. However, that is not often a good thing. If the original sample wasn't closely representative of the population, then the bootstrap distribution mean won't be a good estimate of the population mean. **Bootstrapping cannot correct any potential biases due to differences between the sample and the population.**\n",
    "\n",
    "7. Sample sd vs. bootstrap distribution sd\n",
    "\n",
    "While we do have that limitation in estimating the population mean, one great thing about distributions is that we can also quantify variation. The standard deviation of the sample flavors is around zero-point-three-five-four. Recall that pandas dot-std calculates a sample standard deviation by default. If we calculate the standard deviation of the bootstrap distribution, specifying a ddof of one, then we get a completely different number. So what's going on here?\n",
    "\n",
    "    > coffee_sample['flavor'].std()\n",
    "    \n",
    "    > np.std(bootstrap_distn, ddof=1)\n",
    "    \n",
    "8. Sample, bootstrap dist'n, pop'n standard deviations\n",
    "\n",
    "Remember that one goal of **bootstrapping is to quantify what variability we might expect in our sample statistic as we go from one sample to another**. Recall that this quantity is called the **standard error** as measured by the standard deviation of the sampling distribution of that statistic. The standard deviation of the bootstrap means can be used as a way to estimate this measure of uncertainty. If we multiply that standard error by the square root of the sample size, we get an estimate of the standard deviation in the original population. Our estimate of the standard deviation is around point-three-five-three. The true standard deviation is around point-three-four-one, so our estimate is pretty close. In fact, it is closer than just using the sample standard deviation alone.\n",
    "\n",
    "9. Interpreting the standard errors\n",
    "\n",
    "To recap, the estimated standard error is the standard deviation of the bootstrap distribution values for our statistic of interest. **This estimated standard error times the square root of the sample size gives a really good estimate of the standard deviation of the population.** That is, although bootstrapping was poor at estimating the population mean, it is generally great for estimating the population standard deviation.\n",
    "\n",
    "10. Let's practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e9e00c",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fea39dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a sampling distribution of 2000 replicates.\n",
    "mean_popularity_2000_samp = []\n",
    "\n",
    "# Generate a sampling distribution of 2000 replicates\n",
    "for i in range(2000):\n",
    "    mean_popularity_2000_samp.append(\n",
    "    \t# Sample 500 rows of the population without replacement and calculate the mean popularity. \n",
    "    \tnp.mean(spotify_population.popularity.sample(n=500, replace=False))\n",
    "    )\n",
    "\n",
    "# Print the sampling distribution results\n",
    "print(mean_popularity_2000_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2688ed22",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_popularity_2000_boot = []\n",
    "\n",
    "# Generate a bootstrap distribution of 2000 replicates\n",
    "for _ in range(2000):\n",
    "    mean_popularity_2000_boot.append(\n",
    "    \t# Resample 500 rows and calculate the mean popularity     \n",
    "    \tnp.mean(spotify_sample['popularity'].sample(n=500, replace=True))\n",
    "    )\n",
    "\n",
    "# Print the bootstrap distribution results\n",
    "print(mean_popularity_2000_boot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f84ce7c",
   "metadata": {},
   "source": [
    "*The sampling distribution mean can be used to estimate the population mean, but that is not the case with the bootstrap distribution.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876877a1",
   "metadata": {},
   "source": [
    "Calculate the standard deviation of popularity in 4 ways.\n",
    "\n",
    "* Population: from spotify_population, take the standard deviation of popularity.\n",
    "* Original sample: from spotify_sample, take the standard deviation of popularity.\n",
    "* Sampling distribution: from sampling_distribution, take its standard deviation and multiply by the square root of the sample size (5000).\n",
    "* Bootstrap distribution: from bootstrap_distribution, take its standard deviation and multiply by the square root of the sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdca8ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the population std dev popularity\n",
    "pop_sd = spotify_population['popularity'].std(ddof=0)\n",
    "\n",
    "# Calculate the original sample std dev popularity\n",
    "samp_sd = spotify_sample.popularity.std()\n",
    "\n",
    "# Calculate the sampling dist'n estimate of std dev popularity\n",
    "samp_distn_sd = np.std(sampling_distribution, ddof=1) * np.sqrt(5000)\n",
    "\n",
    "# Calculate the bootstrap dist'n estimate of std dev popularity\n",
    "boot_distn_sd = np.std(bootstrap_distribution, ddof=1) * np.sqrt(5000)\n",
    "\n",
    "# Print the standard deviations\n",
    "print([pop_sd, samp_sd, samp_distn_sd, boot_distn_sd])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254bf9c1",
   "metadata": {},
   "source": [
    "## Confidence intervals\n",
    "1. Confidence intervals\n",
    "\n",
    "In the last few exercises, you looked at relationships between the sampling distribution and the bootstrap distribution.\n",
    "\n",
    "2. Confidence intervals\n",
    "\n",
    "One way to quantify these distributions is the idea of \"values within one standard deviation of the mean\", which gives a good sense of where most of the values in a distribution lie. In this final lesson, we'll formalize the idea of values close to a statistic by defining the term \"confidence interval\".\n",
    "\n",
    "3. Predicting the weather\n",
    "\n",
    "Consider meteorologists predicting weather in one of the world's most unpredictable regions - the northern Great Plains of the US and Canada. Rapid City, South Dakota was ranked as the least predictable of the 120 US cities with a National Weather Service forecast office. Suppose we've taken a job as a meteorologist at a news station in Rapid City. Our job is to predict tomorrow's high temperature.\n",
    "\n",
    "4. Our weather prediction\n",
    "\n",
    "We analyze the weather data using the best forecasting tools available to us and predict a high temperature of 47 degrees Fahrenheit. In this case, 47 degrees is our point estimate. Since the weather is variable, and many South Dakotans will plan their day tomorrow based on our forecast, we'd instead like to present a range of plausible values for the high temperature. On our weather show, we report that the high temperature will be between forty and fifty-four degrees tomorrow.\n",
    "\n",
    "5. We just reported a confidence interval!\n",
    "\n",
    "This prediction of forty to fifty-four degrees can be thought of as a confidence interval for the unknown quantity of tomorrow's high temperature. Although we can't be sure of the exact temperature, we are confident that it will be in that range. These results are often written as the point estimate followed by the confidence interval's lower and upper bounds in parentheses or square brackets. When the confidence interval is symmetric around the point estimate, we can represent it as the point estimate plus or minus the margin of error, in this case, seven degrees.\n",
    "\n",
    "6. Bootstrap distribution of mean flavor\n",
    "\n",
    "Here's the bootstrap distribution of the mean flavor from the coffee dataset.\n",
    "\n",
    "7. Mean of the resamples\n",
    "\n",
    "We can calculate the mean of these resampled mean flavors.\n",
    "\n",
    "8. Mean plus or minus one standard deviation\n",
    "\n",
    "If we create a confidence interval by adding and subtracting one standard deviation from the mean, we see that there are lots of values in the bootstrap distribution outside of this one standard deviation confidence interval.\n",
    "\n",
    "9. Quantile method for confidence intervals\n",
    "\n",
    "If we want to include ninety-five percent of the values in the confidence interval, we can use quantiles. Recall that quantiles split distributions into sections containing a particular proportion of the total data. To get the middle 95% of values, **we go from the 0.025 quantile to the 0.975 quantile** since the difference between those two numbers is point-nine-five. To calculate the lower and upper bounds for this confidence interval, we call quantile from NumPy, passing the distribution values and the quantile values to use. The confidence interval is from around seven-point-four-eight to seven-point-five-four.\n",
    "\n",
    "    > np.quantile(coffee_boot_distn, 0.025)\n",
    "    \n",
    "    > np.quantile(coffee_boot_distn, 0.975)\n",
    "    \n",
    "10. Inverse cumulative distribution function\n",
    "\n",
    "There is a second method to calculate confidence intervals. To understand it, we need to be familiar with the normal distribution's inverse cumulative distribution function. The bell curve we've seen before is the probability density function or PDF. Using calculus, if we integrate this, we get the cumulative distribution function or CDF. If we flip the x and y axes, we get the **inverse CDF**. We can use scipy-dot-stats and call norm-dot-ppf to get the inverse CDF. It takes a quantile between zero and one and returns the values of the normal distribution for that quantile. The parameters of loc and scale are set to 0 and 1 by default, corresponding to the standard normal distribution. Notice that the values corresponding to point-zero-two-five and point-nine-seven-five are about minus and plus two for the standard normal distribution.\n",
    "\n",
    "    > from scipy.stats import norm\n",
    "    \n",
    "    > norm.ppf(quantile, loc=0, scale=1)\n",
    "    \n",
    "11. Standard error method for confidence interval\n",
    "\n",
    "This second method for calculating a confidence interval is called the standard error method. First, we calculate the point estimate, which is the mean of the bootstrap distribution, and the standard error, which is estimated by the standard deviation of the bootstrap distribution. Then we call norm-dot-ppf to get the inverse CDF of the normal distribution with the same mean and standard deviation as the bootstrap distribution. Again, the confidence interval is from seven-point-four-eight to seven-point-five-four, though the numbers differ slightly from last time since our bootstrap distribution isn't perfectly normal.\n",
    "\n",
    "    > point_estimate = np.mean(coffee_boot_distn)\n",
    "    \n",
    "    > std_error = np.std(coffee_boot_distn, ddof=1)\n",
    "    \n",
    "    > lower = norm.ppf(0.025, loc=point_estimate, scale=std_error)\n",
    "\n",
    "    > upper = norm.ppf(0.975, loc=point_estimate, scale=std_error)\n",
    "\n",
    "12. Let's practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1c41f8",
   "metadata": {},
   "source": [
    "*Confidence intervals account for uncertainty in our estimate of a population parameter by providing a range of possible values. We are confident that the true value lies somewhere in the interval specified by that range.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58e8cd4",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5471b12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a 95% confidence interval using the quantile method\n",
    "lower_quant = np.quantile(bootstrap_distribution, 0.025)\n",
    "upper_quant = np.quantile(bootstrap_distribution, 0.975)\n",
    "\n",
    "# Print quantile method confidence interval\n",
    "print((lower_quant, upper_quant))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f25723b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the mean and std dev of the bootstrap distribution\n",
    "point_estimate = np.mean(bootstrap_distribution)\n",
    "standard_error = np.std(bootstrap_distribution, ddof=1)\n",
    "\n",
    "# Find the lower limit of the confidence interval\n",
    "lower_se = norm.ppf(0.025, loc=point_estimate, scale=standard_error)\n",
    "\n",
    "# Find the upper limit of the confidence interval\n",
    "upper_se = norm.ppf(0.975, loc=point_estimate, scale=standard_error)\n",
    "\n",
    "# Print standard error method confidence interval\n",
    "print((lower_se, upper_se))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
