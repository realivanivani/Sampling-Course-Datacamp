{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f91fac08",
   "metadata": {},
   "source": [
    "## Simple random and systematic sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb40bde8",
   "metadata": {},
   "source": [
    "1. Simple random and systematic sampling\n",
    "\n",
    "There are several methods of sampling from a population. In this video, we'll look at simple random sampling and systematic random sampling.\n",
    "\n",
    "2. Simple random sampling\n",
    "\n",
    "Simple random sampling works like a raffle or lottery. We start with our population of raffle tickets or lottery balls and randomly pick them out one at a time.\n",
    "\n",
    "3. Simple random sampling of coffees\n",
    "\n",
    "In our coffee ratings dataset, instead of raffle tickets or lottery balls, the population consists of coffee varieties. To perform simple random sampling, we take some at random, one at a time. Each coffee has the same chance as any other of being picked. When using this technique, sometimes we might end up with two coffees that were next to each other in the dataset, and sometimes we might end up with large areas of the dataset that were not selected from at all.\n",
    "\n",
    "4. Simple random sampling with pandas\n",
    "\n",
    "We've already seen how to do simple random sampling with pandas. We call dot-sample and set n to the size of the sample. We can also set the seed using the random_state argument to generate reproducible results, just like we did pseudo-random number generation. Previously, by not setting random_state when sampling, our code would generate a different random sample each time it was run.\n",
    "\n",
    "5. Systematic sampling\n",
    "\n",
    "Another sampling method is known as systematic sampling. This samples the population at regular intervals. Here, looking from top to bottom and left to right within each row, every fifth coffee is sampled.\n",
    "\n",
    "6. Systematic sampling - defining the interval\n",
    "\n",
    "Systematic sampling with pandas is slightly trickier than simple random sampling. The tricky part is determining how big the interval between each row should be for a given sample size. Suppose we want a sample size of five coffees. The population size is the number of rows in the whole dataset, and in this case, it's one thousand three hundred and thirty-eight. The interval is the population size divided by the sample size, but because we want the answer to be an integer, we perform integer division with two forward slashes. This is like standard division but discards any fractional part. One-three-three-eight divided by five is actually two hundred and sixty-seven-point-six, and discarding the fractional part leaves two hundred and sixty-seven. Thus, to get a systematic sample of five coffees, we will select every two hundred sixty-seventh coffee in the dataset.\n",
    "\n",
    "    > interval = pop_size // sample_size\n",
    "\n",
    "7. Systematic sampling - selecting the rows\n",
    "\n",
    "To select every two hundred and sixty-seventh row, we call dot-iloc on coffee_ratings and pass double-colons and the interval, which is 267 in this case. Double-colon interval tells pandas to select every two hundred and sixty-seventh row from zero to the end of the DataFrame.\n",
    "\n",
    "    > cofee_rating.iloc[::interval]\n",
    "\n",
    "8. The trouble with systematic sampling\n",
    "\n",
    "There is a problem with systematic sampling, though. Suppose we are interested in statistics about the aftertaste attribute of the coffees. To examine this, first, we use reset_index to create a column of index values in our DataFrame that we can plot. Plotting aftertaste against index shows a pattern. Earlier rows generally have higher aftertaste scores than later rows. This introduces bias into the statistics that we calculate. In general, it is only safe to use systematic sampling if a plot like this has no pattern; that is, it just looks like noise.\n",
    "\n",
    "9. Making systematic sampling safe\n",
    "\n",
    "To ensure that systematic sampling is safe, we can randomize the row order before sampling. dot-sample has an argument named frac that lets us specify the proportion of the dataset to return in the sample, rather than the absolute number of rows that n specifies. Setting frac to one randomly samples the whole dataset. In effect, this randomly shuffles the rows. Next, the indices need to be reset so that they go in order from zero again. Specifying drop equals True clears the previous row indexes, and chaining to another reset_index call creates a column containing these new indexes. Redrawing the plot with the shuffled dataset shows no pattern between aftertaste and index. This is great, but note that once we've shuffled the rows, systematic sampling is essentially the same as simple random sampling.\n",
    "\n",
    "    > shuffled = coffee_ratings.sample(frac=1)\n",
    "    \n",
    "    > shuffled = shuffled.reset_index(drop=True).reset_index()\n",
    "\n",
    "10. Let's practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2016d9b9",
   "metadata": {},
   "source": [
    "#### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28cb260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 70 rows from attrition_pop using simple random sampling, setting the random seed to 18900217.\n",
    "attrition_samp = attrition_pop.sample(n=70, random_state=18900217)\n",
    "\n",
    "# Print the sample\n",
    "print(attrition_samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644124ee",
   "metadata": {},
   "source": [
    "One sampling method that avoids randomness is called systematic sampling. Here, you pick rows from the population at regular intervals.\n",
    "\n",
    "For example, if the population dataset had one thousand rows, and you wanted a sample size of five, you could pick rows 0, 200, 400, 600, and 800.\n",
    "\n",
    "attrition_pop is available; pandas has been pre-loaded as pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b99e144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the sample size to 70\n",
    "sample_size = 70\n",
    "\n",
    "# Calculate the population size from attrition_pop\n",
    "pop_size = len(attrition_pop)\n",
    "\n",
    "# Calculate the interval\n",
    "interval = pop_size // sample_size\n",
    "\n",
    "# Systematically sample 70 rows\n",
    "attrition_sys_samp = attrition_pop.iloc[::interval]\n",
    "\n",
    "# Print the sample\n",
    "print(attrition_sys_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86182483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an index column to attrition_pop, assigning the result to attrition_pop_id.\n",
    "# Add an index column to attrition_pop\n",
    "attrition_pop_id = attrition_pop.reset_index(drop=True).reset_index()\n",
    "\n",
    "# Create a scatter plot of YearsAtCompany versus index for attrition_pop_id using pandas .plot().\n",
    "# Plot YearsAtCompany vs. index for attrition_pop_id\n",
    "attrition_pop_id.plot(x='index', y='YearsAtCompany', kind='scatter')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Randomly shuffle the rows of attrition_pop.\n",
    "# Shuffle the rows of attrition_pop\n",
    "attrition_shuffled = attrition_pop.sample(frac=1)\n",
    "\n",
    "# Reset the row indexes, and add an index column to attrition_pop.\n",
    "# Reset the row indexes and create an index column\n",
    "attrition_shuffled = attrition_shuffled.reset_index(drop=True).reset_index()\n",
    "\n",
    "# Repeat the scatter plot of YearsAtCompany versus index, this time using attrition_shuffled.\n",
    "# Plot YearsAtCompany vs. index for attrition_shuffled\n",
    "attrition_shuffled.plot(x='index', y='YearsAtCompany', kind='scatter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb640c38",
   "metadata": {},
   "source": [
    "## Stratified and weighted random sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607b236a",
   "metadata": {},
   "source": [
    "1. Stratified and weighted random sampling\n",
    "\n",
    "Stratified sampling is a technique that allows us to sample a population that contains subgroups.\n",
    "\n",
    "2. Coffees by country\n",
    "\n",
    "For example, we could group the coffee ratings by country. If we count the number of coffees by country using the value_counts method, we can see that these six countries have the most data.\n",
    "\n",
    "1 The dataset lists Hawaii and Taiwan as countries for convenience, as they are notable coffee-growing regions.\n",
    "\n",
    "3. Filtering for 6 countries\n",
    "\n",
    "To make it easier to think about sampling subgroups, let's limit our analysis to these six countries. We can use the dot-isin method to filter the population and only return the rows corresponding to these six countries. This filtered dataset is stored as coffee_ratings_top.\n",
    "\n",
    "4. Counts of a simple random sample\n",
    "\n",
    "Let's take a ten percent simple random sample of the dataset using dot-sample with frac set to zero-point-one. We also set the random_state argument to ensure reproducibility. As with the whole dataset, we can look at the counts for each country. To make comparisons easier, we set normalize to True to convert the counts into a proportion, which shows what proportion of coffees in the sample came from each country.\n",
    "\n",
    "5. Comparing proportions\n",
    "\n",
    "Here are the proportions for the population and the ten percent sample side by side. Just by chance, in this sample, Taiwanese coffees form a disproportionately low percentage. The different makeup of the sample compared to the population could be a problem if we want to analyze the country of origin, for example.\n",
    "\n",
    "6. Proportional stratified sampling\n",
    "\n",
    "If we care about the proportions of each country in the sample closely matching those in the population, then we can group the data by country before taking the simple random sample. Note that we used the Python line continuation backslash here, which can be useful for breaking up longer chains of pandas code like this. Calling the dot-sample method after grouping takes a simple random sample within each country. Now the proportions of each country in the stratified sample are much closer to those in the population.\n",
    "\n",
    "    > coffee_ratings_top.groupby(\"country_of_origin\").sample(frac=0.1, random_state=2021)\n",
    "    \n",
    "7. Equal counts stratified sampling\n",
    "\n",
    "One variation of stratified sampling is to sample equal counts from each group, rather than an equal proportion. The code only has one change from before. This time, we use the n argument in dot-sample instead of frac to extract fifteen randomly-selected rows from each country. Here, the resulting sample has equal proportions of one-sixth from each country.\n",
    "\n",
    "    > coffee_ratings_eq.groupby(\"country_of_origin\").sample(n=15, random_state=2021)\n",
    "\n",
    "8. Weighted random sampling\n",
    "\n",
    "A close relative of stratified sampling that provides even more flexibility is weighted random sampling. In this variant, we create a column of weights that adjust the relative probability of sampling each row. For example, suppose we thought that it was important to have a higher proportion of Taiwanese coffees in the sample than in the population. We create a condition, in this case, rows where the country of origin is Taiwan. Using the where function from NumPy, we can set a weight of two for rows that match the condition and a weight of one for rows that don't match the condition. This means when each row is randomly sampled, Taiwanese coffees have two times the chance of being picked compared to other coffees. When we call dot-sample, we pass the column of weights to the weights argument.\n",
    "\n",
    "    > condition = coffee_rating_weight['country_of_origin'] == 'Taiwan'\n",
    "\n",
    "    > coffee_ratings_weights['weight'] = np.where(condition, 2, 1)\n",
    "    \n",
    "    > coffee_ratings_weight = coffee_ratings_weight.sample(frac=0.1, weights=\"weight\")\n",
    "    \n",
    "9. Weighted random sampling results\n",
    "\n",
    "Here, we can see that Taiwan now contains seventeen percent of the sampled dataset, compared to eight-point-five percent in the population. This sort of weighted sampling is common in political polling, where we need to correct for under- or over-representation of demographic groups.\n",
    "\n",
    "10. Let's practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02803a41",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18712583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the proportion of employees by Education level from attrition_pop.\n",
    "education_counts_pop = attrition_pop['Education'].value_counts(normalize=True)\n",
    "\n",
    "# Proportional stratified sampling for 40% of each Education group\n",
    "attrition_strat = attrition_pop.groupby('Education').sample(frac=0.4, random_state=2022)\n",
    "\n",
    "# Calculate the Education level proportions from attrition_strat\n",
    "education_counts_strat = attrition_strat['Education'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d524cf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use equal counts stratified sampling on attrition_pop to get 30 employees from each Education group, setting the seed to 2022.\n",
    "attrition_eq = attrition_pop.groupby('Education').sample(n=30, random_state=2022)\n",
    "\n",
    "# Get the proportions from attrition_eq\n",
    "education_counts_eq = attrition_eq['Education'].value_counts(normalize=True)\n",
    "\n",
    "# Plot YearsAtCompany from attrition_pop as a histogram with bins of width 1 from 0 to 40.\n",
    "attrition_pop['YearsAtCompany'].hist(bins=np.arange(0,41,1))\n",
    "plt.show()\n",
    "\n",
    "# Sample 400 employees weighted by YearsAtCompany\n",
    "attrition_weight = attrition_pop.sample(n=400, weights='YearsAtCompany')\n",
    "\n",
    "# Plot YearsAtCompany from attrition_weight as a histogram\n",
    "attrition_weight['YearsAtCompany'].hist(bins=np.arange(0, 41, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad3401e",
   "metadata": {},
   "source": [
    "## Cluster sampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d508de8d",
   "metadata": {},
   "source": [
    "1. Cluster sampling\n",
    "\n",
    "One problem with stratified sampling is that we need to collect data from every subgroup. In cases where collecting data is expensive, for example, when we have to physically travel to a location to collect it, it can make our analysis prohibitively expensive. There's a cheaper alternative called cluster sampling.\n",
    "\n",
    "2. Stratified sampling vs. cluster sampling\n",
    "\n",
    "The stratified sampling approach was to split the population into subgroups, then use simple random sampling on each of them. Cluster sampling means that we limit the number of subgroups in the analysis by picking a few of them with simple random sampling. We then perform simple random sampling on each subgroup as before.\n",
    "\n",
    "3. Varieties of coffee\n",
    "\n",
    "Let's return to the coffee dataset and look at the varieties of coffee. In this image, each bean represents the whole subgroup rather than an individual coffee, and there are twenty-eight of them. To extract unique varieties, we use the dot-unique method. This returns an array, so wrapping it in the list function creates a list of unique varieties. Let's suppose that it's expensive to work with all of the different varieties. Enter cluster sampling.\n",
    "\n",
    "4. Stage 1: sampling for subgroups\n",
    "\n",
    "The first stage of cluster sampling is to randomly cut down the number of varieties, and we do this by randomly selecting them. Here, we've used the random-dot-sample function from the random package to get three varieties, specified using the argument k.\n",
    "\n",
    "    > varieties_samp = random.sample(varieties_pop, k=3)\n",
    "    \n",
    "5. Stage 2: sampling each group\n",
    "\n",
    "The second stage of cluster sampling is to perform simple random sampling on each of the three varieties we randomly selected. We first filter the dataset for rows where the variety is one of the three selected, using the dot-isin method. To ensure that the isin filtering removes levels with zero rows, we apply the cat-dot-remove_unused_categories method on the Series of focus, which is variety here. If we exclude this method, we might receive an error when sampling by variety level. The pandas code is the same as for stratified sampling. Here, we've opted for equal counts sampling, with five rows from each remaining variety.\n",
    "    \n",
    "    > variety_conditions = coffee_ratings['variety'].isin(varieties_samp)\n",
    "    \n",
    "    > coffeee_ratings_cluster = coffee_ratings[variety_conditions]\n",
    "    \n",
    "    > coffeee_ratings_cluster['variety'] = coffeee_ratings_cluster['variety'].cat.remove_unused_categories()\n",
    "    \n",
    "    > coffeee_ratings_cluster.groupby('variety').sample(n=5, random_state=2021)\n",
    "    \n",
    "6. Stage 2 output\n",
    "\n",
    "Here's the first few columns of the result. Notice that there are the fifteen rows, which we'd expect from sampling five rows from three varieties.\n",
    "\n",
    "7. Multistage sampling\n",
    "\n",
    "Note that we had two stages in the cluster sampling. We randomly sampled the subgroups to include, then we randomly sampled rows from those subgroups. Cluster sampling is a special case of multistage sampling. It's possible to use more than two stages. A common example is national surveys, which can include several levels of administrative regions, like states, counties, cities, and neighborhoods.\n",
    "\n",
    "8. Let's practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751f6d96",
   "metadata": {},
   "source": [
    "*Cluster sampling is a two-stage sampling technique that is closely related to stratified sampling. First, you randomly sample which subgroups to include in the sample, then randomly sample rows within each subgroup.*\n",
    "\n",
    "In which of the following situations would cluster sampling be preferable to stratified sampling?\n",
    "\n",
    "*The main benefit of cluster sampling over stratified sampling is that you can save time and money by not including every subgroup in your sample.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6010f581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Create a list of unique JobRole values\n",
    "job_roles_pop = list(attrition_pop.JobRole.unique())\n",
    "\n",
    "# Randomly sample four JobRole values\n",
    "job_roles_samp = random.sample(job_roles_pop, k=4)\n",
    "\n",
    "# Subset attrition_pop for the sampled job roles by filtering for rows where JobRole is in job_roles_samp.\n",
    "jobrole_condition = attrition_pop['JobRole'].isin(job_roles_samp)\n",
    "attrition_filtered = attrition_pop[jobrole_condition]\n",
    "\n",
    "# Remove categories with no rows\n",
    "attrition_filtered['JobRole'] = attrition_filtered['JobRole'].cat.remove_unused_categories()\n",
    "\n",
    "# For each job role in the filtered dataset, take a random sample of ten rows, setting the seed to 2022\n",
    "# Randomly sample 10 employees from each sampled job role\n",
    "attrition_clust = attrition_filtered.groupby('JobRole').sample(n=10, random_state=2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871fac8b",
   "metadata": {},
   "source": [
    "## Comparing sampling methods\n",
    "\n",
    "1. Comparing sampling methods\n",
    "\n",
    "2. Review of sampling techniques - setup\n",
    "\n",
    "For convenience, we'll stick to the six countries with the most coffee varieties that we used before. This corresponds to eight hundred and eighty rows and eight columns, retrieved using the dot-shape attribute.\n",
    "\n",
    "3. Review of simple random sampling\n",
    "\n",
    "Simple random sampling uses dot-sample with either n or frac set to determine how many rows to pseudo-randomly choose, given a seed value set with random_state. The simple random sample returns two hundred and ninety-three rows because we specified frac as one-third, and one-third of eight hundred and eighty is just over two hundred and ninety-three.\n",
    "\n",
    "4. Review of stratified sampling\n",
    "\n",
    "Stratified sampling groups by the country subgroup before performing simple random sampling on each subgroup. Given each of these top countries have quite a few rows, stratifying produces the same number of rows as the simple random sample.\n",
    "\n",
    "5. Review of cluster sampling\n",
    "\n",
    "In the cluster sample, we've used two out of six countries to roughly mimic frac equals one-third from the other sample types. Setting n equal to one-sixth of the total number of rows gives roughly equal sample sizes in each of the two subgroups. Using dot-shape again, we see that this cluster sample has close to the same number of rows: two-hundred-ninety-two compared to two-hundred-ninety-three for the other sample types.\n",
    "\n",
    "6. Calculating mean cup points\n",
    "\n",
    "Let's calculate a population parameter, the mean of the total cup points. When the population parameter is the mean of a field, it's often called the population mean. Remember that in real-life scenarios, we typically wouldn't know what the population mean is. Since we have it here, though, we can use this value of eighty-one-point-nine as a gold standard to measure against. Now we'll calculate the same value using each of the sampling techniques we've discussed. These are point estimates of the mean, often called sample means. The simple and stratified sample means are really close to the population mean. Cluster sampling isn't quite as close, but that's typical. Cluster sampling is designed to give us an answer that's almost as good while using less data.\n",
    "\n",
    "7. Mean cup points by country: simple random\n",
    "\n",
    "Here's a slightly more complicated calculation of the mean total cup points for each country. We group by country before calculating the mean to return six numbers. So how do the numbers from the simple random sample compare? The sample means are pretty close to the population means.\n",
    "\n",
    "8. Mean cup points by country: stratified\n",
    "\n",
    "The same is true of the sample means from the stratified technique. Each sample mean is relatively close to the population mean.\n",
    "\n",
    "9. Mean cup points by country: cluster\n",
    "\n",
    "With cluster sampling, while the sample means are pretty close to the population means, the obvious limitation is that we only get values for the two countries that were included in the sample. If the mean cup points for each country is an important metric in our analysis, cluster sampling would be a bad idea.\n",
    "\n",
    "10. Let's practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6b8f62",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a57eb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform simple random sampling to get 0.25 of the population\n",
    "attrition_srs = attrition_pop.sample(frac=0.25, random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76ccf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform stratified sampling to get 0.25 of each relationship group\n",
    "attrition_strat = attrition_pop.groupby('RelationshipSatisfaction').sample(frac=0.25, random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fa207b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of unique RelationshipSatisfaction values\n",
    "satisfaction_unique = list(attrition_pop.RelationshipSatisfaction.unique())\n",
    "\n",
    "# Randomly sample 2 unique satisfaction values\n",
    "satisfaction_samp = random.sample(satisfaction_unique, k=2)\n",
    "\n",
    "# Filter for satisfaction_samp and clear unused categories from RelationshipSatisfaction\n",
    "satis_condition = attrition_pop.RelationshipSatisfaction.isin(satisfaction_samp)\n",
    "attrition_clust_prep = attrition_pop[satis_condition]\n",
    "attrition_clust_prep['RelationshipSatisfaction'] = attrition_clust_prep['RelationshipSatisfaction'].cat.remove_unused_categories()\n",
    "\n",
    "# Perform cluster sampling on the selected group, getting 0.25 of attrition_pop\n",
    "attrition_clust = attrition_clust_prep.groupby('RelationshipSatisfaction').sample(n=len(attrition_pop)//4, random_state=2022)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f13a9f",
   "metadata": {},
   "source": [
    "### Comparing point estimates\n",
    "Now that you have three types of sample (simple, stratified, and cluster), you can compare point estimates from each sample to the population parameter. That is, you can calculate the same summary statistic on each sample and see how it compares to the summary statistic for the population.\n",
    "\n",
    "Here, we'll look at how satisfaction with the company affects whether or not the employee leaves the company. That is, you'll calculate the proportion of employees who left the company (they have an Attrition value of 1) for each value of RelationshipSatisfaction.\n",
    "\n",
    "attrition_pop, attrition_srs, attrition_strat, and attrition_clust are available; pandas is loaded with its usual alias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afc6abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Attrition by RelationshipSatisfaction group\n",
    "mean_attrition_pop = attrition_pop.groupby('RelationshipSatisfaction').Attrition.mean()\n",
    "\n",
    "# Calculate the same thing for the simple random sample \n",
    "mean_attrition_srs = attrition_srs.groupby('RelationshipSatisfaction').Attrition.mean()\n",
    "\n",
    "# Calculate the same thing for the stratified sample \n",
    "mean_attrition_strat = attrition_strat.groupby('RelationshipSatisfaction').Attrition.mean()\n",
    "\n",
    "# Calculate the same thing for the cluster sample \n",
    "mean_attrition_clust = attrition_clust.groupby('RelationshipSatisfaction').Attrition.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
